# Project-6
Задачи проекта:
Разведывательный анализ и очистка данных:

Провести анализ данных, выявить и устранить дублирующиеся категории, ошибки ввода, жаргонные сокращения и другие аномалии.
Обработка пропусков и удаление выбросов в данных.
Выделение наиболее значимых факторов:

Определить ключевые факторы, влияющие на стоимость недвижимости.
Построение модели:

Разработка модели машинного обучения с использованием алгоритма Random Forest для прогнозирования стоимости недвижимости.
Разработка веб-сервиса:

Создать веб-сервис, который на основе входных данных о недвижимости будет прогнозировать её стоимость.
Структура проекта
Проект состоит из следующих основных этапов:

Подготовка данных:

Загрузка и первичная очистка данных.
Удаление повторяющихся строк и ненужных столбцов.
Разведывательный анализ данных (EDA):

Анализ и визуализация распределений различных признаков.
Определение и обработка выбросов и пропущенных значений.
Моделирование:

Разделение данных на обучающую и тестовую выборки.
Настройка модели Random Forest и оптимизация её гиперпараметров с помощью GridSearchCV.
Оценка качества модели на тестовой выборке с использованием метрики RMSE (Среднеквадратическая ошибка).
Разработка веб-сервиса:

Реализация простого веб-интерфейса для предсказания стоимости недвижимости.
Для запуска проекта вам потребуется установить следующие библиотеки:

pip install pandas
pip install matplotlib
pip install seaborn
pip install scikit-learn
pip install joblib
pip install streamlit
Использование
Запустите Jupyter Notebook и выполните шаги, описанные в проекте.
Для запуска веб-сервиса используйте Streamlit:
streamlit run app.py

 описания кода 
 Импорт необходимых библиотек
pandas: для работы с данными в табличном формате.
re: для работы с регулярными выражениями (используется для обработки текстовых данных).
matplotlib.pyplot и seaborn: для визуализации данных.
sklearn.model_selection: для разделения данных на обучающие и тестовые выборки, а также для настройки гиперпараметров.
sklearn.ensemble: для использования модели Random Forest.
sklearn.metrics: для оценки качества модели (в данном случае используется метрика mean_squared_error).

Подготовка данных
# Преобразование текстовых данных в числовой формат для модели
df['baths'] = df['baths'].apply(lambda x: float(re.search(r'\d+\.?\d*', x).group()) if pd.notnull(x) else x)
df['sqft'] = df['sqft'].apply(lambda x: float(re.search(r'\d+\.?\d*', x).group().replace(',', '')) if pd.notnull(x) else x)
df['beds'] = df['beds'].apply(lambda x: float(re.search(r'\d+\.?\d*', x).group()) if pd.notnull(x) else x)
df['target'] = df['target'].apply(lambda x: float(x.replace('$', '').replace(',', '')) if pd.notnull(x) else x)
Здесь происходит преобразование текстовых данных в числовой формат, который может быть использован для обучения модели:

baths, sqft, beds, и target содержат текстовые значения с цифрами и другими символами. С помощью регулярных выражений происходит извлечение числовых значений и их преобразование в числовой тип данных.

Разделение данных на обучающую и тестовую выборки

# Разделение данных на признаки и целевую переменную
X = df.drop(columns=['target'])
y = df['target']

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

На этом этапе данные разделяются на признаки (X) и целевую переменную (y). Затем, эти данные разделяются на обучающую (80%) и тестовую (20%) выборки.

Обучение модели Random Forest и подбор гиперпараметров

# Обучение модели Random Forest
model = RandomForestRegressor(random_state=42)

# Подбор гиперпараметров с помощью GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Лучшая модель
best_model = grid_search.best_estimator_

Здесь происходит:

Обучение модели: Создается модель RandomForestRegressor.
Подбор гиперпараметров: Используется GridSearchCV для поиска наилучших параметров модели. Этот процесс включает в себя кросс-валидацию, которая помогает выбрать оптимальные значения гиперпараметров (например, количество деревьев в лесу, максимальная глубина деревьев и т.д.).
Выбор лучшей модели: Лучшая модель сохраняется в переменной best_model.

Оценка модели
# Предсказание на тестовой выборке
y_pred = best_model.predict(X_test)

# Оценка качества модели
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5

print(f"Root Mean Squared Error: {rmse}")

На этом этапе модель оценивается на тестовой выборке:

Предсказание: С помощью модели делаются предсказания для тестовой выборки.
Оценка качества: Вычисляется среднеквадратичная ошибка (MSE) и её квадратный корень (RMSE), который является основной метрикой оценки модели.
